{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2ef2835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scrapy in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.12.0)\n",
      "Requirement already satisfied: selenium in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: requests in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: Twisted>=21.7.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (24.11.0)\n",
      "Requirement already satisfied: cryptography>=37.0.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (43.0.3)\n",
      "Requirement already satisfied: cssselect>=0.9.1 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (1.3.0)\n",
      "Requirement already satisfied: itemloaders>=1.0.1 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (1.3.2)\n",
      "Requirement already satisfied: parsel>=1.5.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (1.10.0)\n",
      "Requirement already satisfied: pyOpenSSL>=22.0.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (25.0.0)\n",
      "Requirement already satisfied: queuelib>=1.4.2 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (1.8.0)\n",
      "Requirement already satisfied: service-identity>=18.1.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (24.2.0)\n",
      "Requirement already satisfied: w3lib>=1.17.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (2.3.1)\n",
      "Requirement already satisfied: zope.interface>=5.1.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (7.2)\n",
      "Requirement already satisfied: protego>=0.1.15 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (0.4.0)\n",
      "Requirement already satisfied: itemadapter>=0.1.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (0.11.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (24.2)\n",
      "Requirement already satisfied: tldextract in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=4.6.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (5.3.2)\n",
      "Requirement already satisfied: defusedxml>=0.7.1 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (0.7.1)\n",
      "Requirement already satisfied: PyDispatcher>=2.0.5 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scrapy) (2.0.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.20)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (4.12.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from beautifulsoup4) (2.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cryptography>=37.0.0->scrapy) (1.17.1)\n",
      "Requirement already satisfied: jmespath>=0.9.5 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (24.2.0)\n",
      "Requirement already satisfied: pyasn1 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from service-identity>=18.1.0->scrapy) (0.4.1)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\amirm\\appdata\\roaming\\python\\python39\\site-packages (from trio~=0.17->selenium) (1.2.2)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: automat>=24.8.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from Twisted>=21.7.0->scrapy) (24.7.2)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from zope.interface>=5.1.0->scrapy) (78.1.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tldextract->scrapy) (2.1.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tldextract->scrapy) (3.16.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cffi>=1.12->cryptography>=37.0.0->scrapy) (2.22)\n",
      "Requirement already satisfied: tomli in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from incremental>=24.7.0->Twisted>=21.7.0->scrapy) (2.2.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\amirm\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scrapy selenium requests beautifulsoup4 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1fc522b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "import urllib3\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Disable insecure request warnings (use with caution)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# --- Constants ---\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Accept-Language\": \"fa-IR,fa;q=0.9,en-US;q=0.8,en;q=0.7\"\n",
    "}\n",
    "\n",
    "PERSIAN_DIGITS = str.maketrans('۰۱۲۳۴۵۶۷۸۹', '0123456789')\n",
    "\n",
    "PERSIAN_NUMBERS = {\n",
    "    ' صفر ': 0, 'یک': 1, ' دو ': 2, ' سه ': 3, ' چهار ': 4,\n",
    "    ' پنج ': 5, ' شش ': 6, ' هفت ': 7, ' هشت ': 8, ' نه' : 9,\n",
    "    ' ده ': 10, ' یازده ': 11, ' دوازده ': 12, ' سیزده ': 13,\n",
    "    ' چهارده ': 14, ' پانزده ': 15, ' شانزده ': 16, ' هفده ': 17,\n",
    "    ' هجده ': 18, ' نوزده ': 19, ' بیست ': 20, ' سی ': 30,\n",
    "    ' چهل ': 40, ' پنجاه ': 50, ' شصت ': 60, ' هفتاد ': 70,\n",
    "    ' هشتاد ': 80, ' نود ': 90, ' صد ': 100,\n",
    "    ' یک چهارم ': 0.25, ' نصف ': 0.5, ' سه چهارم ': 0.75\n",
    "}\n",
    "\n",
    "UNITS = [\n",
    "    'قاشق غذاخوری', 'قاشق چایخوری', 'کیلوگرم', 'گرم',\n",
    "    'پیمانه', 'عدد', 'لیتر', 'فنجان', 'حبه', 'قاشق',\n",
    "    'تکه', 'به مقدار لازم', 'مقدار لازم'\n",
    "]\n",
    "\n",
    "# --- Utility Functions ---\n",
    "\n",
    "\n",
    "def convert_persian_numbers(text):\n",
    "    \"\"\"Converts Persian digits and number words in a string to standard format.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text  # Return non-strings as is\n",
    "    # First convert digits\n",
    "    text = text.translate(PERSIAN_DIGITS)\n",
    "    for word, value in PERSIAN_NUMBERS.items():\n",
    "        text = text.replace(word, str(value))\n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_title(title):\n",
    "    \"\"\"Removes leading numbers/hyphens and converts Persian numbers in a title.\"\"\"\n",
    "    title = title.strip()\n",
    "    # Remove patterns like \"1-\", \"2.\", \"۳-\" etc. from the beginning\n",
    "    cleaned = re.sub(r\"^[\\d۰۱۲۳۴۵۶۷۸۹]+[-–.ـ\\s]*\", \"\", title).strip()\n",
    "    return convert_persian_numbers(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc80df",
   "metadata": {},
   "source": [
    "Isfahan Province, Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34696d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 28 food entries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "URL = \"https://jainjas.com/Blog/414/%D8%BA%D8%B0%D8%A7%D9%87%D8%A7%DB%8C-%D9%85%D8%AD%D9%84%DB%8C-%D8%A7%D8%B5%D9%81%D9%87%D8%A7%D9%86\"\n",
    "\n",
    "# Category configuration\n",
    "CATEGORY_MAPPING = {\n",
    "    'گوشتی': {\n",
    "        'meal_type': ['غذای اصلی'],\n",
    "        'occasion': ['ناهار', 'شام']\n",
    "    },\n",
    "    'حلیم': {\n",
    "        'meal_type': ['پیش غذا'],\n",
    "        'occasion': ['صبحانه', 'عصرانه']\n",
    "    },\n",
    "    'آش': {\n",
    "        'meal_type': ['پیش غذا'],\n",
    "        'occasion': ['صبحانه', 'عصرانه']\n",
    "    },\n",
    "    'پلو خورش': {\n",
    "        'meal_type': ['دسر'],\n",
    "        'occasion': ['ناهار', 'شام']\n",
    "    },\n",
    "    'شیرین': {\n",
    "        'meal_type': ['دسر'],\n",
    "        'occasion': ['ناهار', 'شام']\n",
    "    }\n",
    "}\n",
    "\n",
    "def detect_category(text):\n",
    "    \"\"\"Detect category from heading text\"\"\"\n",
    "    text = text.lower()\n",
    "    for keyword in CATEGORY_MAPPING:\n",
    "        if keyword in text:\n",
    "            return keyword\n",
    "    return None\n",
    "\n",
    "\n",
    "def parse_amount_unit(amount_text):\n",
    "    amount_text = convert_persian_numbers(amount_text.strip())\n",
    "\n",
    "    if any(phrase in amount_text for phrase in ['کافی', 'لازم', 'مقدار']):\n",
    "        return (\"مقدار لازم\", \"مقدار لازم\")\n",
    "\n",
    "    for word, value in PERSIAN_NUMBERS.items():\n",
    "        if word in amount_text:\n",
    "            unit = amount_text.replace(word, '').strip()\n",
    "            return (value, unit)\n",
    "\n",
    "    numeric_chars = []\n",
    "    for c in amount_text:\n",
    "        if c.isdigit() or c in ',./':\n",
    "            numeric_chars.append(c)\n",
    "        elif numeric_chars:\n",
    "            break\n",
    "\n",
    "    numeric_part = ''.join(numeric_chars)\n",
    "    unit_part = amount_text.replace(numeric_part, '').strip()\n",
    "\n",
    "    if '/' in numeric_part:\n",
    "        parts = numeric_part.split('/')\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                value = float(parts[0]) / float(parts[1])\n",
    "                return (value, unit_part)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    numeric_part = numeric_part.replace(',', '.')\n",
    "\n",
    "    try:\n",
    "        return (float(numeric_part), unit_part)\n",
    "    except:\n",
    "        return (\"مقدار لازم\", amount_text)\n",
    "\n",
    "\n",
    "def parse_ingredients(table):\n",
    "    ingredients = []\n",
    "    for row in table.find_all('tr'):\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) != 2:\n",
    "            continue\n",
    "\n",
    "        name = cols[0].get_text(strip=True)\\\n",
    "                      .replace('\\xa0', ' ')\\\n",
    "                      .replace('‌', ' ')\\\n",
    "                      .strip()\n",
    "\n",
    "        amount_text = cols[1].get_text(strip=True)\n",
    "        amount, unit = parse_amount_unit(amount_text)\n",
    "\n",
    "        ingredients.append({\n",
    "            \"name\": name,\n",
    "            \"amount\": amount,\n",
    "            \"unit\": unit\n",
    "        })\n",
    "\n",
    "    return ingredients\n",
    "\n",
    "\n",
    "def scrape_foods():\n",
    "    try:\n",
    "        response = requests.get(URL, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        foods = []\n",
    "        current_config = None\n",
    "\n",
    "        for element in soup.find_all(['h2', 'h3']):\n",
    "            # Process category headings\n",
    "            if element.name == 'h2':\n",
    "                detected = detect_category(element.get_text(strip=True))\n",
    "                if detected:\n",
    "                    current_config = CATEGORY_MAPPING[detected]\n",
    "                continue\n",
    "\n",
    "            # Process food items\n",
    "            if element.name == 'h3' and element.get('id', '').startswith('sec-'):\n",
    "                if not current_config:\n",
    "                    continue\n",
    "\n",
    "                title = element.get_text(strip=True)\\\n",
    "                    .replace('\\xa0', ' ')\\\n",
    "                    .replace('‌', ' ')\\\n",
    "                    .strip()\n",
    "\n",
    "                if not title or title == \"انواع آش در اصفهان\":\n",
    "                    continue\n",
    "\n",
    "                # Get related elements\n",
    "                ingredients_table = element.find_next('table')\n",
    "                instructions = []\n",
    "                images = []\n",
    "\n",
    "                # Process instructions\n",
    "                next_element = ingredients_table.find_next_sibling() if ingredients_table else None\n",
    "                while next_element and next_element.name not in ['h2', 'h3']:\n",
    "                    if next_element.name == 'p':\n",
    "                        instruction = next_element.get_text(strip=True)\n",
    "                        if instruction:\n",
    "                            # Extract steps using regex (supports \"1.\", \"2-\", \"3)\", etc.)\n",
    "                            steps = re.findall(\n",
    "                                r'\\d+[\\.\\-)]\\s*(.*?)(?=\\s*\\d+[\\.\\-)]|$)', \n",
    "                                instruction, \n",
    "                                flags=re.DOTALL\n",
    "                            )\n",
    "                            for step in steps:\n",
    "                                cleaned_step = step.strip()\n",
    "                                if cleaned_step:\n",
    "                                    instructions.append(cleaned_step)\n",
    "                    next_element = next_element.find_next_sibling()\n",
    "\n",
    "                # Process images\n",
    "                img = element.find_next('img')\n",
    "                while img and img.find_previous('h3') == element:\n",
    "                    if 'src' in img.attrs:\n",
    "                        images.append(img['src'])\n",
    "                    img = img.find_next('img')\n",
    "\n",
    "                food = {\n",
    "                    \"title\": title,\n",
    "                    \"location\": {\n",
    "                        \"province\": \"اصفهان\",\n",
    "                        \"city\": \"اصفهان\",\n",
    "                        \"coordinates\": {\n",
    "                            \"latitude\": 32.6539,\n",
    "                            \"longitude\": 51.6660\n",
    "                        }\n",
    "                    },\n",
    "                    \"ingredients\": parse_ingredients(ingredients_table) if ingredients_table else [],\n",
    "                    \"instructions\": instructions,\n",
    "                    \"meal_type\": current_config['meal_type'],\n",
    "                    \"occasion\": current_config['occasion'],\n",
    "                    \"images\": {\n",
    "                        \"تصویر نهایی\": images[0] if images else \"\",\n",
    "                        **{f\"{i} مرحله\": url for i, url in enumerate(images[1:], start=1)}\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                if food[\"ingredients\"]:\n",
    "                    foods.append(food)\n",
    "\n",
    "        return foods\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Scraping failed: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "# Execute and save\n",
    "if __name__ == \"__main__\":\n",
    "    foods_data = scrape_foods()\n",
    "\n",
    "    with open(\"isfahan_foods.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(foods_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Successfully saved {len(foods_data)} food entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e301755",
   "metadata": {},
   "source": [
    "Shiraz Province, Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "70429f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amirm\\AppData\\Local\\Temp\\ipykernel_11096\\2272519454.py:13: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  for a in soup.find_all('a', text=lambda text: text and 'طرز تهیه' in text):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 recipe URLs to scrape\n",
      "Successfully saved 32 food entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def scrape_recipe_urls():\n",
    "    \"\"\"Scrape recipe URLs containing 'طرز تهیه' from the blog page\"\"\"\n",
    "    try:\n",
    "        url = \"https://blog.okcs.com/shiraz-foods-desserts/\"\n",
    "        response = requests.get(url, headers=headers, verify=False)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        base_url = \"https://blog.okcs.com\"\n",
    "        recipe_urls = []\n",
    "\n",
    "        # Find all links containing 'طرز تهیه' in their text\n",
    "        for a in soup.find_all('a', text=lambda text: text and 'طرز تهیه' in text):\n",
    "            href = a.get('href')\n",
    "            if href:\n",
    "                # Convert relative URLs to absolute\n",
    "                if not href.startswith(('http://', 'https://')):\n",
    "                    href = f\"{base_url}{href}\" if href.startswith(\n",
    "                        '/') else f\"{base_url}/{href}\"\n",
    "                recipe_urls.append(href)\n",
    "\n",
    "        # Remove duplicates while preserving order\n",
    "        seen = set()\n",
    "        return [x for x in recipe_urls if not (x in seen or seen.add(x))]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping recipe URLs: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def parse_amount_unit(amount_text):\n",
    "    amount_text = convert_persian_numbers(amount_text.strip())\n",
    "\n",
    "    if any(phrase in amount_text for phrase in ['کافی', 'لازم', 'مقدار']):\n",
    "        return (\"مقدار لازم\", \"مقدار لازم\")\n",
    "\n",
    "    for word, value in PERSIAN_NUMBERS.items():\n",
    "        if word in amount_text:\n",
    "            unit = amount_text.replace(word, '').strip()\n",
    "            return (value, unit)\n",
    "\n",
    "    numeric_chars = []\n",
    "    for c in amount_text:\n",
    "        if c.isdigit() or c in ',./':\n",
    "            numeric_chars.append(c)\n",
    "        elif numeric_chars:\n",
    "            break\n",
    "    numeric_part = ''.join(numeric_chars)\n",
    "    unit_part = amount_text.replace(numeric_part, '').strip()\n",
    "\n",
    "    if '/' in numeric_part:\n",
    "        parts = numeric_part.split('/')\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                value = float(parts[0]) / float(parts[1])\n",
    "                return (value, unit_part)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    numeric_part = numeric_part.replace(',', '.')\n",
    "    try:\n",
    "        return (float(numeric_part), unit_part)\n",
    "    except Exception:\n",
    "        return (\"مقدار لازم\", amount_text)\n",
    "\n",
    "\n",
    "def parse_ingredients_from_table(table):\n",
    "    ingredients = []\n",
    "    rows = table.find_all('tr')\n",
    "    if rows and \"مواد اولیه\" in rows[0].get_text():\n",
    "        rows = rows[1:]\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) != 2:\n",
    "            continue\n",
    "        name = cols[0].get_text(strip=True).replace(\n",
    "            '\\xa0', ' ').replace('‌', ' ').strip()\n",
    "        amount_text = cols[1].get_text(strip=True)\n",
    "        amount, unit = parse_amount_unit(amount_text)\n",
    "        ingredients.append({\n",
    "            \"name\": name,\n",
    "            \"amount\": amount,\n",
    "            \"unit\": unit\n",
    "        })\n",
    "    return ingredients\n",
    "\n",
    "\n",
    "def parse_ingredients_from_ul(ul):\n",
    "    ingredients = []\n",
    "    for li in ul.find_all(\"li\"):\n",
    "        text = li.get_text(strip=True)\n",
    "        if ':' in text:\n",
    "            parts = text.split(\":\", 1)\n",
    "            name = parts[0].strip()\n",
    "            amount_text = parts[1].strip()\n",
    "            amount, unit = parse_amount_unit(amount_text)\n",
    "            ingredients.append({\n",
    "                \"name\": name,\n",
    "                \"amount\": amount,\n",
    "                \"unit\": unit\n",
    "            })\n",
    "    return ingredients\n",
    "\n",
    "\n",
    "def scrape_recipe_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        title_tag = soup.find(\"strong\")\n",
    "        food_title = title_tag.get_text(strip=True) if title_tag else \"نامشخص\"\n",
    "\n",
    "        # Find image at top of post\n",
    "        image_url = \"\"\n",
    "        featured_img = soup.find(\"img\", class_=\"attachment-post-thumbnail\")\n",
    "        if featured_img and featured_img.has_attr(\"src\"):\n",
    "            image_url = featured_img[\"src\"]\n",
    "\n",
    "        ingredients = []\n",
    "        table_found = None\n",
    "        for t in soup.find_all(\"table\"):\n",
    "            if \"مواد اولیه\" in t.get_text():\n",
    "                table_found = t\n",
    "                break\n",
    "        if table_found:\n",
    "            ingredients = parse_ingredients_from_table(table_found)\n",
    "        else:\n",
    "            for ul in soup.find_all(\"ul\"):\n",
    "                if any(':' in li.get_text() for li in ul.find_all(\"li\")):\n",
    "                    ingredients = parse_ingredients_from_ul(ul)\n",
    "                    if ingredients:\n",
    "                        break\n",
    "\n",
    "        instructions = []\n",
    "        for h3 in soup.find_all(\"h3\"):\n",
    "            span = h3.find(\"span\")\n",
    "            if span:\n",
    "                text = span.get_text(strip=True)\n",
    "                if text.startswith(\"مرحله\"):\n",
    "                    instructions.append(text)\n",
    "\n",
    "        return {\n",
    "            \"title\": food_title,\n",
    "            \"ingredients\": ingredients,\n",
    "            \"instructions\": instructions,\n",
    "            \"image\": image_url\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping recipe page {url}: {str(e)}\")\n",
    "        return {\"title\": \"نامشخص\", \"ingredients\": [], \"instructions\": [], \"image\": \"\"}\n",
    "\n",
    "\n",
    "def scrape_shiraz_foods(recipe_urls):\n",
    "    foods = []\n",
    "    for url in recipe_urls:\n",
    "        recipe_data = scrape_recipe_page(url)\n",
    "        food_item = {\n",
    "            \"title\": recipe_data.get(\"title\", \"نامشخص\"),\n",
    "            \"location\": {\n",
    "                \"province\": \"فارس\",\n",
    "                \"city\": \"شیراز\",\n",
    "                \"coordinates\": {\n",
    "                    \"latitude\": 29.5926,\n",
    "                    \"longitude\": 52.5836\n",
    "                }\n",
    "            },\n",
    "            \"ingredients\": recipe_data.get(\"ingredients\", []),\n",
    "            \"instructions\": recipe_data.get(\"instructions\", []),\n",
    "            \"meal_type\": [\"اصلی\", \"دسر\", \"پیش غذا\"],\n",
    "            \"occasion\": [\"شام\", \"ناهار\", \"صبحانه\"],\n",
    "            \"images\": {\n",
    "                \"تصویر نهایی\": recipe_data.get(\"image\", \"\")\n",
    "            }\n",
    "        }\n",
    "        foods.append(food_item)\n",
    "    return foods\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    recipe_urls = scrape_recipe_urls()\n",
    "    if not recipe_urls:\n",
    "        print(\"No recipe URLs found. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Found {len(recipe_urls)} recipe URLs to scrape\")\n",
    "\n",
    "    foods_data = scrape_shiraz_foods(recipe_urls)\n",
    "    with open(\"shiraz_foods.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(foods_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Successfully saved {len(foods_data)} food entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb01dc6b",
   "metadata": {},
   "source": [
    "Hormozgan Province, Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a56aaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Hormozgan local foods successfully saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAIN_PAGE_URL = \"https://blog.okala.com/hormozgan-cuisine/\"\n",
    "\n",
    "def parse_amount_unit(amount_text):\n",
    "    amount_text = convert_persian_numbers(amount_text.strip())\n",
    "\n",
    "    if any(word in amount_text for word in [\"کافی\", \"لازم\", \"مقدار\"]):\n",
    "        return (\"مقدار لازم\", \"مقدار لازم\")\n",
    "\n",
    "    for word, value in PERSIAN_NUMBERS.items():\n",
    "        if word in amount_text:\n",
    "            unit = amount_text.replace(word, '').strip()\n",
    "            return (value, unit)\n",
    "\n",
    "    match = re.match(r'([\\d./]+)\\s*(.*)', amount_text)\n",
    "    if match:\n",
    "        number, unit = match.groups()\n",
    "        if '/' in number:\n",
    "            parts = number.split('/')\n",
    "            if len(parts) == 2:\n",
    "                try:\n",
    "                    value = float(parts[0]) / float(parts[1])\n",
    "                    return (value, unit)\n",
    "                except Exception:\n",
    "                    pass\n",
    "        number = number.replace(',', '.')\n",
    "        try:\n",
    "            return (float(number), unit)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return (\"مقدار لازم\", amount_text)\n",
    "\n",
    "\n",
    "def parse_ingredients_from_table(table):\n",
    "    ingredients = []\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) != 2:\n",
    "            continue\n",
    "        name = cols[0].get_text(strip=True)\n",
    "        amount_text = cols[1].get_text(strip=True)\n",
    "        amount, unit = parse_amount_unit(amount_text)\n",
    "        ingredients.append({\n",
    "            \"name\": name,\n",
    "            \"amount\": amount,\n",
    "            \"unit\": unit\n",
    "        })\n",
    "    return ingredients\n",
    "\n",
    "\n",
    "def scrape_recipe_page(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        ingredients = []\n",
    "        for table in soup.find_all(\"table\"):\n",
    "            if \"مواد\" in table.get_text() and \"لازم\" in table.get_text():\n",
    "                ingredients = parse_ingredients_from_table(table)\n",
    "                break\n",
    "\n",
    "        instructions = []\n",
    "        for h3 in soup.find_all(\"h3\"):\n",
    "            text = h3.get_text(strip=True)\n",
    "            if text.startswith(\"مرحله\"):\n",
    "                instructions.append(text)\n",
    "\n",
    "        return {\n",
    "            \"ingredients\": ingredients,\n",
    "            \"instructions\": instructions\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping recipe page {url}: {e}\")\n",
    "        return {\n",
    "            \"ingredients\": [],\n",
    "            \"instructions\": []\n",
    "        }\n",
    "\n",
    "\n",
    "def scrape_hormozgan_foods():\n",
    "    foods = []\n",
    "    image_urls = [\n",
    "        \"https://blog.okala.com/wp-content/uploads/2023/12/pudini-kooseh-1-1.jpg\",\n",
    "        \"https://blog.okala.com/wp-content/uploads/2023/12/muflek-hormozgan-1-1.jpg\",\n",
    "        \"https://blog.okala.com/wp-content/uploads/2023/12/mahyave-ba-mahi-khoshk-1-1.jpg\",\n",
    "        \"https://blog.okala.com/wp-content/uploads/2023/12/koofte-mahi-moomgh-3-1.jpg\",\n",
    "        \"https://blog.okala.com/wp-content/uploads/2023/12/katoogh-mahi-1-1.jpg\",\n",
    "        \"https://blog.okala.com/wp-content/uploads/2023/12/mahi-gariz-kababi-1.jpg\",\n",
    "        \"https://blog.okala.com/wp-content/uploads/2024/02/dopiyaze-meygoo.jpg\"\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(MAIN_PAGE_URL, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        h2_tags = soup.find_all(\"h2\")\n",
    "        food_index = 0\n",
    "\n",
    "        for h2 in h2_tags:\n",
    "            span = h2.find(\"span\")\n",
    "            if not span:\n",
    "                continue\n",
    "\n",
    "            title_raw = h2.get_text(strip=True)\n",
    "            title_clean = clean_title(title_raw)\n",
    "\n",
    "            image_url = image_urls[food_index]\n",
    "\n",
    "            a_tag = h2.find_next(\"a\", href=True)\n",
    "            if not a_tag:\n",
    "                continue\n",
    "\n",
    "            relative_url = a_tag[\"href\"]\n",
    "            recipe_url = urljoin(MAIN_PAGE_URL, relative_url)\n",
    "\n",
    "            if not recipe_url.startswith(\"https://blog.okala.com\"):\n",
    "                continue\n",
    "\n",
    "            recipe_data = scrape_recipe_page(recipe_url)\n",
    "\n",
    "            food_item = {\n",
    "                \"title\": title_clean,\n",
    "                \"location\": {\n",
    "                    \"province\": \"هرمزگان\",\n",
    "                    \"city\": \"بندرعباس\",\n",
    "                    \"coordinates\": {\n",
    "                        \"latitude\": 27.1963,\n",
    "                        \"longitude\": 56.2884\n",
    "                    }\n",
    "                },\n",
    "                \"ingredients\": recipe_data.get(\"ingredients\", []),\n",
    "                \"instructions\": recipe_data.get(\"instructions\", []),\n",
    "                \"meal_type\": [\"اصلی\", \"دسر\", \"پیش غذا\"],\n",
    "                \"occasion\": [\"شام\", \"ناهار\", \"صبحانه\"],\n",
    "                \"images\": {\n",
    "                    \"تصویر نهایی\": image_url\n",
    "                }\n",
    "            }\n",
    "            foods.append(food_item)\n",
    "            food_index += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping main page: {e}\")\n",
    "    return foods\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    foods_data = scrape_hormozgan_foods()\n",
    "    with open(\"hormozgan_foods.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(foods_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{len(foods_data)} Hormozgan local foods successfully saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe07be",
   "metadata": {},
   "source": [
    "Chaharmahal and Bakhtiari Province, Iran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6eb676e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 3 food items.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "MAIN_PAGE_URL = \"https://www.bartarinha.ir/بخش-آشپزی-15/1152168-محبوب-ترین-غذا-های-محلی-ایران\"\n",
    "\n",
    "def parse_ingredient(li_text):\n",
    "    li_text = convert_persian_numbers(li_text.strip())\n",
    "    if ': ' in li_text:\n",
    "        name, amount = li_text.split(': ', 1)\n",
    "    else:\n",
    "        name, amount = li_text, \"مقدار لازم\"\n",
    "\n",
    "    # Convert numeric values\n",
    "    amount_match = re.match(r\"([\\d.]+)(.*)\", amount.strip())\n",
    "    if amount_match:\n",
    "        numeric = float(amount_match.group(1)) if '.' in amount_match.group(\n",
    "            1) else int(amount_match.group(1))\n",
    "        unit = amount_match.group(2).strip()\n",
    "    else:\n",
    "        numeric = amount.strip()\n",
    "        unit = \"\"\n",
    "\n",
    "    return {\n",
    "        \"name\": name.strip(),\n",
    "        \"amount\": numeric if isinstance(numeric, (int, float)) else amount,\n",
    "        \"unit\": unit\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_foods():\n",
    "    foods = []\n",
    "    current_food = None\n",
    "    in_recipe = False\n",
    "\n",
    "    try:\n",
    "        response = requests.get(MAIN_PAGE_URL, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        for element in soup.find_all(True):\n",
    "            # Start new food entry\n",
    "            if element.name == 'strong' and not current_food:\n",
    "                current_food = {\n",
    "                    \"title\": clean_title(element.get_text()),\n",
    "                    \"location\": {\n",
    "                        \"province\": \"چهارمحال و بختیاری\",\n",
    "                        \"city\": \"شهرکرد\",\n",
    "                        \"coordinates\": {\n",
    "                            \"latitude\": 32.3274,\n",
    "                            \"longitude\": 50.8650\n",
    "                        }\n",
    "                    },\n",
    "                    \"ingredients\": [],\n",
    "                    \"instructions\": [],\n",
    "                    \"meal_type\": [\"ناهار\", \"شام\"],\n",
    "                    \"occasion\": [\"روزمره\"],\n",
    "                    \"images\": {\"تصویر نهایی\": \"\"}\n",
    "                }\n",
    "                continue\n",
    "\n",
    "            if current_food:\n",
    "                # Get image\n",
    "                if element.name == 'img' and not current_food['images']['تصویر نهایی']:\n",
    "                    current_food['images']['تصویر نهایی'] = element.get(\n",
    "                        'src', '')\n",
    "\n",
    "                # Get ingredients\n",
    "                elif element.name == 'strong' and 'مواد' in element.get_text():\n",
    "                    ul = element.find_next('ul')\n",
    "                    if ul:\n",
    "                        current_food['ingredients'] = [parse_ingredient(\n",
    "                            li.get_text()) for li in ul.find_all('li')]\n",
    "\n",
    "                # Start recipe collection\n",
    "                elif element.name == 'strong' and ('طرز' in element.get_text() or 'نکات' in element.get_text()):\n",
    "                    in_recipe = True\n",
    "                    # SPECIAL CASE: If it's \"نکات مهم\", stop immediately\n",
    "                    if 'نکات مهم' in element.get_text():\n",
    "                        in_recipe = False\n",
    "                        continue\n",
    "\n",
    "                # Collect recipe steps\n",
    "                elif in_recipe and element.name == 'p':\n",
    "                    current_food['instructions'].append(\n",
    "                        element.get_text(strip=True))\n",
    "\n",
    "                # End recipe collection on new food title\n",
    "                elif element.name == 'strong' and not ('مواد' in element.get_text() or 'طرز' in element.get_text()):\n",
    "                    foods.append(current_food)\n",
    "                    current_food = {\n",
    "                        \"title\": clean_title(element.get_text()),\n",
    "                        \"location\": {\n",
    "                            \"province\": \"چهارمحال و بختیاری\",\n",
    "                            \"city\": \"شهرکرد\",\n",
    "                            \"coordinates\": {\n",
    "                                \"latitude\": 32.3265,\n",
    "                                \"longitude\": 50.8644\n",
    "                            }\n",
    "                        },\n",
    "                        \"ingredients\": [],\n",
    "                        \"instructions\": [],\n",
    "                        \"meal_type\": [\"اصلی\", \"دسر\", \"پیش غذا\"],\n",
    "                        \"occasion\": [\"شام\", \"ناهار\", \"صبحانه\"],\n",
    "                        \"images\": {\"تصویر نهایی\": \"\"}\n",
    "                    }\n",
    "                    in_recipe = False\n",
    "\n",
    "        # Add the last food item\n",
    "        if current_food:\n",
    "            foods.append(current_food)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return foods\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    foods_data = scrape_foods()\n",
    "    with open(\"chaharmahal_foods.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(foods_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Successfully saved {len(foods_data)} food items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d747785d",
   "metadata": {},
   "source": [
    "Khusestan Province, Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fd4c4d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 79 Khuzestan food entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_amount_unit(text):\n",
    "    original_text = text\n",
    "    text = convert_persian_numbers(text.strip())\n",
    "\n",
    "    # Handle \"مقدار لازم\" cases\n",
    "    if any(phrase in text for phrase in ['به مقدار لازم', 'مقدار لازم']):\n",
    "        name = re.sub(r'(به? مقدار لازم)', '', text,\n",
    "                      flags=re.IGNORECASE).strip()\n",
    "        unit = 'به مقدار لازم' if 'به مقدار لازم' in text else 'مقدار لازم'\n",
    "        return (\"مقدار لازم\", unit, name)\n",
    "\n",
    "    # Check for Persian number words with word boundaries\n",
    "    for word, value in sorted(PERSIAN_NUMBERS.items(), key=lambda x: -len(x[0])):\n",
    "        if re.search(rf'\\b{word}\\b', text):\n",
    "            parts = re.split(rf'\\b{word}\\b', text, 1)\n",
    "            remaining = parts[-1].strip()\n",
    "            for unit in sorted(UNITS, key=lambda x: -len(x)):\n",
    "                if remaining.startswith(unit):\n",
    "                    return (value, unit, remaining[len(unit):].strip())\n",
    "            return (value, remaining, '')\n",
    "\n",
    "    # Extract numeric values with proper decimal handling\n",
    "    num_match = re.match(r'^(\\d+/\\d+|\\d+[\\.,]?\\d*)\\s*', text)\n",
    "    if num_match:\n",
    "        num_str = num_match.group(1).replace(',', '.')\n",
    "        remaining = text[len(num_str):].strip()\n",
    "\n",
    "        try:\n",
    "            if '/' in num_str:\n",
    "                numerator, denominator = map(float, num_str.split('/'))\n",
    "                amount = numerator / denominator\n",
    "            else:\n",
    "                amount = float(num_str)\n",
    "        except:\n",
    "            return (\"مقدار لازم\", \"مقدار لازم\", original_text)\n",
    "\n",
    "        # Find longest matching unit\n",
    "        for unit in sorted(UNITS, key=lambda x: -len(x)):\n",
    "            if remaining.startswith(unit):\n",
    "                return (amount, unit, remaining[len(unit):].strip())\n",
    "\n",
    "        # If no unit found, take first word as unit\n",
    "        if remaining:\n",
    "            unit_match = re.match(r'^(\\S+)', remaining)\n",
    "            if unit_match:\n",
    "                unit = unit_match.group(1)\n",
    "                return (amount, unit, remaining[len(unit):].strip())\n",
    "\n",
    "        return (amount, '', remaining)\n",
    "\n",
    "    return (\"مقدار لازم\", \"مقدار لازم\", original_text)\n",
    "\n",
    "\n",
    "def scrape_khuzestan_foods():\n",
    "    BASE_URL = \"https://www.kojaro.com/food/117017-khouzestan-traditional-food--part-1/\"\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        foods = []\n",
    "        current_food = None\n",
    "        capture_instructions = False\n",
    "        skip_first = True\n",
    "\n",
    "        elements = soup.find_all(['h2', 'h3', 'h4', 'img', 'ul', 'p'])\n",
    "\n",
    "        for element in elements:\n",
    "            if element.name == 'h2' and 'طعم غذاهای محلی خوزستان' in element.get_text():\n",
    "                break\n",
    "\n",
    "            if element.name == 'h3':\n",
    "                if skip_first:\n",
    "                    skip_first = False\n",
    "                    continue\n",
    "\n",
    "                if current_food:\n",
    "                    foods.append(current_food)\n",
    "\n",
    "                current_food = {\n",
    "                    \"title\": element.get_text(strip=True),\n",
    "                    \"location\": {\n",
    "                        \"province\": \"خورستان\",\n",
    "                        \"city\": \"اهواز\",\n",
    "                        \"coordinates\": {\n",
    "                            \"latitude\": 31.318327,\n",
    "                            \"longitude\": 48.670620\n",
    "                        }\n",
    "                    },\n",
    "                    \"ingredients\": [],\n",
    "                    \"instructions\": [],\n",
    "                    \"meal_type\": [\"اصلی\", \"دسر\", \"پیش غذا\"],\n",
    "                    \"occasion\": [\"شام\", \"ناهار\", \"صبحانه\"],\n",
    "                    \"images\": {\"تصویر نهایی\": \"\"}\n",
    "                }\n",
    "\n",
    "            elif current_food:\n",
    "                if element.name == 'img' and not current_food['images'][\"تصویر نهایی\"]:\n",
    "                    src = element.get('src', '')\n",
    "                    current_food['images'][\"تصویر نهایی\"] = urljoin(BASE_URL, src)\n",
    "                    \n",
    "                    \n",
    "\n",
    "                if element.name == 'h4' and 'مواد لازم' in element.get_text():\n",
    "                    ul = element.find_next('ul')\n",
    "                    if ul:\n",
    "                        for li in ul.find_all('li'):\n",
    "                            text = li.get_text(strip=True)\n",
    "                            amount, unit, name = parse_amount_unit(text)\n",
    "\n",
    "                            # Fallback for unparsed names\n",
    "                            if not name and unit not in ['مقدار لازم', 'به مقدار لازم']:\n",
    "                                name = text\n",
    "\n",
    "                            current_food['ingredients'].append({\n",
    "                                \"name\": name.strip(),\n",
    "                                \"amount\": amount,\n",
    "                                \"unit\": unit.strip()\n",
    "                            })\n",
    "\n",
    "                if element.name == 'h4' and 'طرز تهیه' in element.get_text():\n",
    "                    capture_instructions = True\n",
    "                    current_food['instructions'] = []\n",
    "                elif capture_instructions:\n",
    "                    if element.name == 'p':\n",
    "                        paragraph = element.get_text(strip=True)\n",
    "                        # Split sentences while preserving numbering\n",
    "                        sentences = []\n",
    "                        buffer = []\n",
    "                        for part in re.split(r'(?<=[\\.\\d])\\s+', paragraph):\n",
    "                            if re.match(r'^\\d+\\.', part):\n",
    "                                if buffer:\n",
    "                                    sentences.append(' '.join(buffer))\n",
    "                                buffer = [part]\n",
    "                            else:\n",
    "                                buffer.append(part)\n",
    "                        if buffer:\n",
    "                            sentences.append(' '.join(buffer))\n",
    "                        current_food['instructions'].extend(sentences)\n",
    "                    elif element.name in ['h3', 'h4']:\n",
    "                        capture_instructions = False\n",
    "\n",
    "        if current_food:\n",
    "            foods.append(current_food)\n",
    "\n",
    "        # Post-process ingredients\n",
    "        for food in foods:\n",
    "            for ing in food['ingredients']:\n",
    "                # Cleanup for \"مقدار لازم\" cases\n",
    "                if ing['unit'] in ['مقدار لازم', 'به مقدار لازم']:\n",
    "                    ing['name'] = ing['name'].replace(\n",
    "                        'به مقدار لازم', '').strip()\n",
    "                    ing['name'] = ing['name'].replace('مقدار لازم', '').strip()\n",
    "\n",
    "                # Remove any remaining numbers from name\n",
    "                ing['name'] = re.sub(r'^\\d+\\s*', '', ing['name']).strip()\n",
    "\n",
    "        return foods\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    khuzestan_foods = scrape_khuzestan_foods()\n",
    "\n",
    "    with open(\"khuzestan_foods.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(khuzestan_foods, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(khuzestan_foods)} Khuzestan food entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ba79ef",
   "metadata": {},
   "source": [
    "Bushehr Province, Iran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "feec5a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping duplicate title: قلیه میگو بوشهری\n",
      "Skipping duplicate title: خورش ماهی بوشهری\n",
      "Successfully saved 28 Bushehr foods.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "COOKPAD_BUSHEHR_URL = \"https://cookpad.com/ir/جستجو/بوشهر\"\n",
    "\n",
    "def parse_ingredient(li):\n",
    "    amount_tag = li.find('bdi', class_='font-semibold')\n",
    "    name_tag = li.find('span')\n",
    "\n",
    "    amount = amount_tag.get_text(strip=True) if amount_tag else \"مقدار لازم\"\n",
    "    name = name_tag.get_text(strip=True) if name_tag else \"\"\n",
    "\n",
    "    # Convert amounts\n",
    "    amount = convert_persian_numbers(amount)\n",
    "    amount_match = re.match(r\"([\\d.]+)(.*)\", amount)\n",
    "    if amount_match:\n",
    "        numeric = float(amount_match.group(1)) if '.' in amount_match.group(\n",
    "            1) else int(amount_match.group(1))\n",
    "        unit = amount_match.group(2).strip()\n",
    "    else:\n",
    "        numeric = amount\n",
    "        unit = \"\"\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"amount\": numeric if isinstance(numeric, (int, float)) else amount,\n",
    "        \"unit\": unit\n",
    "    }\n",
    "\n",
    "\n",
    "def scrape_bushehr_cookpad():\n",
    "    foods = []\n",
    "    seen_titles = set()  # Track titles to avoid duplicates\n",
    "\n",
    "    try:\n",
    "        # First get main page with food links\n",
    "        main_response = requests.get(\n",
    "            COOKPAD_BUSHEHR_URL, headers=headers, verify=False)\n",
    "        main_soup = BeautifulSoup(main_response.content, \"html.parser\")\n",
    "\n",
    "        # Get all food links\n",
    "        food_links = main_soup.select('a.block-link__main[href^=\"/ir/\"]')\n",
    "\n",
    "        for link in food_links[:30]:  # Limit to 30 links\n",
    "            # Extract and clean title from the link text\n",
    "            title = clean_title(link.get_text(strip=True))\n",
    "\n",
    "            # Skip duplicate titles\n",
    "            if title in seen_titles:\n",
    "                print(f\"Skipping duplicate title: {title}\")\n",
    "                continue\n",
    "            seen_titles.add(title)\n",
    "\n",
    "            food_url = urljoin(COOKPAD_BUSHEHR_URL, link['href'])\n",
    "\n",
    "            # Scrape individual food page\n",
    "            food_response = requests.get(\n",
    "                food_url, headers=headers, verify=False)\n",
    "            food_soup = BeautifulSoup(food_response.content, \"html.parser\")\n",
    "\n",
    "            food_data = {\n",
    "                \"title\": title,  # Use pre-cleaned title\n",
    "                \"location\": {\n",
    "                    \"province\": \"بوشهر\",\n",
    "                    \"city\": \"بوشهر\",\n",
    "                    \"coordinates\": {\n",
    "                        \"latitude\": 28.9145,\n",
    "                        \"longitude\": 50.8279\n",
    "                    }\n",
    "                },\n",
    "                \"ingredients\": [],\n",
    "                \"instructions\": [],\n",
    "                \"meal_type\": [\"اصلی\", \"دسر\",\"پیش غذا\"],\n",
    "                \"occasion\": [\"شام\",\"ناهار\",\"صبحانه\"],\n",
    "                \"images\": {\"تصویر نهایی\": \"\"}\n",
    "            }\n",
    "\n",
    "            # Get main image\n",
    "            main_img = food_soup.find('img', {'fetchpriority': 'high'})\n",
    "            if main_img:\n",
    "                food_data[\"images\"][\"تصویر نهایی\"] = main_img.get('src', '')\n",
    "\n",
    "            # Get ingredients\n",
    "            ingredient_list = food_soup.find('div', class_='ingredient-list')\n",
    "            if ingredient_list:\n",
    "                food_data[\"ingredients\"] = [parse_ingredient(li)\n",
    "                                            for li in ingredient_list.find_all('li')]\n",
    "\n",
    "            # Get recipe steps\n",
    "            steps_section = food_soup.find('ol', class_='list-none')\n",
    "            if steps_section:\n",
    "                for step in steps_section.find_all('li', class_='step'):\n",
    "                    text_div = step.find('div', dir='auto')\n",
    "                    if text_div:\n",
    "                        food_data[\"instructions\"].append(\n",
    "                            text_div.get_text(strip=True))\n",
    "\n",
    "            foods.append(food_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    return foods\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bushehr_foods = scrape_bushehr_cookpad()\n",
    "    with open(\"bushehr_foods.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(bushehr_foods, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Successfully saved {len(bushehr_foods)} Bushehr foods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4318322",
   "metadata": {},
   "source": [
    " Kohgiluyeh and Boyer-Ahmad Province, Iran\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b846a69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved 9 food entries.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def scrape_blogfa_foods():\n",
    "    URL = \"https://aadabkb79.blogfa.com/post/5\"\n",
    "    try:\n",
    "        response = requests.get(URL, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        foods = []\n",
    "        h1_tags = soup.find_all('h1')\n",
    "\n",
    "        counter = 0\n",
    "        for h1 in h1_tags:\n",
    "            if(counter == 0):\n",
    "                counter += 1\n",
    "                continue\n",
    "            food_item = {\n",
    "                \"title\": h1.get_text(strip=True),\n",
    "                \"location\": {\n",
    "                    \"province\": \"کهگیلویه و بویراحمد\",\n",
    "                    \"city\": \"یاسوج\",\n",
    "                    \"coordinates\": {\n",
    "                        \"latitude\": 30.6638,\n",
    "                        \"longitude\": 51.5949\n",
    "                    }\n",
    "                },\n",
    "                \"ingredients\": [],\n",
    "                \"instructions\": [],\n",
    "                \"meal_type\": [\"اصلی\", \"دسر\", \"پیش غذا\"],\n",
    "                \"occasion\": [\"شام\", \"ناهار\", \"صبحانه\"],\n",
    "                \"images\": {\"تصویر نهایی\": \"\"}\n",
    "            }\n",
    "\n",
    "            # Find the recipe div following the h1\n",
    "            recipe_div = h1.find_next('div')\n",
    "\n",
    "            if recipe_div:\n",
    "                # Extract text from all paragraphs\n",
    "                paragraphs = recipe_div.find_all('p')\n",
    "                instructions = []\n",
    "                for p in paragraphs:\n",
    "                    if p.find('img'):\n",
    "                        continue  # Skip paragraphs containing images\n",
    "                    text = p.get_text(strip=True)\n",
    "                    if text:\n",
    "                        sentences = [s.strip()\n",
    "                                     for s in text.split('.') if s.strip()]\n",
    "                        instructions.extend(sentences)\n",
    "\n",
    "                food_item['instructions'] = instructions\n",
    "\n",
    "                # # Find image if exists\n",
    "                # img = recipe_div.find('img')\n",
    "                # if img and img.has_attr('src'):\n",
    "                #     food_item['images']['تصویر نهایی'] = img['src']\n",
    "\n",
    "            foods.append(food_item)\n",
    "\n",
    "        return foods\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    food_data = scrape_blogfa_foods()\n",
    "\n",
    "    with open(\"kohgiluyeh_foods.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(food_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Successfully saved {len(food_data)} food entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6813a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting concatenation process...\n",
      "Reading data from isfahan_foods.json...\n",
      "-> Added 28 entries from isfahan_foods.json\n",
      "Reading data from shiraz_foods.json...\n",
      "-> Added 32 entries from shiraz_foods.json\n",
      "Reading data from hormozgan_foods.json...\n",
      "-> Added 6 entries from hormozgan_foods.json\n",
      "Reading data from chaharmahal_foods.json...\n",
      "-> Added 3 entries from chaharmahal_foods.json\n",
      "Reading data from khuzestan_foods.json...\n",
      "-> Added 79 entries from khuzestan_foods.json\n",
      "Reading data from bushehr_foods.json...\n",
      "-> Added 28 entries from bushehr_foods.json\n",
      "Reading data from kohgiluyeh_foods.json...\n",
      "-> Added 9 entries from kohgiluyeh_foods.json\n",
      "\n",
      "Successfully combined a total of 185 food entries into Local_Foods.json.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "input_json_files = [\n",
    "    \"isfahan_foods.json\",\n",
    "    \"shiraz_foods.json\",\n",
    "    \"hormozgan_foods.json\",\n",
    "    \"chaharmahal_foods.json\",\n",
    "    \"khuzestan_foods.json\",\n",
    "    \"bushehr_foods.json\",  \n",
    "    \"kohgiluyeh_foods.json\"\n",
    "]\n",
    "\n",
    "output_json_file = \"Local_Foods.json\"\n",
    "\n",
    "combined_food_data = []\n",
    "total_entries = 0\n",
    "\n",
    "print(\"Starting concatenation process...\")\n",
    "\n",
    "# Loop through each input file\n",
    "for file_path in input_json_files:\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                print(f\"Reading data from {file_path}...\")\n",
    "                data = json.load(f)\n",
    "\n",
    "                if isinstance(data, list):\n",
    "                    combined_food_data.extend(data)\n",
    "                    print(f\"-> Added {len(data)} entries from {file_path}\")\n",
    "                    total_entries += len(data)\n",
    "                else:\n",
    "                    print(\n",
    "                        f\"Warning: Content of {file_path} is not a list. Skipping.\")\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(\n",
    "                f\"Error: Could not decode JSON from {file_path}. File might be corrupted or empty. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"An error occurred while reading {file_path}: {e}. Skipping.\")\n",
    "    else:\n",
    "        print(f\"Warning: File not found - {file_path}. Skipping.\")\n",
    "\n",
    "if combined_food_data:\n",
    "    try:\n",
    "        with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(combined_food_data, f, ensure_ascii=False, indent=2)\n",
    "        print(\n",
    "            f\"\\nSuccessfully combined a total of {total_entries} food entries into {output_json_file}.\")\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            f\"\\nError writing the final combined file {output_json_file}: {e}\")\n",
    "else:\n",
    "    print(\"\\nNo data was collected from the input files. Output file not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdfc585",
   "metadata": {},
   "source": [
    "Cleaning the LabelStudio Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f175175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from 'LabelStudio_Output.json'...\n",
      "Loaded 185 entries.\n",
      "Filtering out 'بد' quality entries and removing metadata...\n",
      "Filtering complete. Kept 170 entries, removed 15 entries.\n",
      "Saving filtered and cleaned data to 'Filered_Data.json'...\n",
      "Save complete.\n",
      "\n",
      "--- Analyzing Final Data ---\n",
      "Total word count in 'Filered_Data.json': 26529\n",
      "Average instruction length in 'Filered_Data.json' (words per recipe with instructions): 119.46\n",
      "Number of recipes with instructions considered for average: 170\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re  \n",
    "import copy  \n",
    "\n",
    "input_json_file = 'LabelStudio_Output.json'  \n",
    "output_json_file = 'Filered_Data.json'     \n",
    "\n",
    "KEYS_TO_REMOVE = {\n",
    "    \"id\",\n",
    "    \"quality\",       # Will be removed after being used for filtering\n",
    "    \"annotator\",\n",
    "    \"annotation_id\",\n",
    "    \"created_at\",\n",
    "    \"updated_at\",\n",
    "    \"lead_time\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Counts words in a given string, handling non-strings and splitting robustly.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    words = re.split(r'\\s+', text.strip())\n",
    "    return len([word for word in words if word])\n",
    "\n",
    "\n",
    "original_data = []\n",
    "processed_data = []  \n",
    "\n",
    "try:\n",
    "    print(f\"Loading data from '{input_json_file}'...\")\n",
    "    with open(input_json_file, 'r', encoding='utf-8') as f:\n",
    "        original_data = json.load(f)\n",
    "\n",
    "    if isinstance(original_data, dict):\n",
    "        original_data = [original_data]\n",
    "    elif not isinstance(original_data, list):\n",
    "        print(\n",
    "            f\"Error: Expected JSON root to be an object or a list, but got {type(original_data)}\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Loaded {len(original_data)} entries.\")\n",
    "\n",
    "    print(\"Filtering out 'بد' quality entries and removing metadata...\")\n",
    "    original_items_count = len(original_data)\n",
    "    filtered_out_count = 0\n",
    "\n",
    "    for index, item in enumerate(original_data):\n",
    "        if not isinstance(item, dict):\n",
    "            print(\n",
    "                f\"Warning: Skipping item at original index {index} because it's not a dictionary (type: {type(item)}). Value: {item}\")\n",
    "            filtered_out_count += 1  \n",
    "            continue\n",
    "\n",
    "        if item.get(\"quality\") == \"بد\":\n",
    "            filtered_out_count += 1\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            item_for_output = item.copy()\n",
    "            for key_to_remove in KEYS_TO_REMOVE:\n",
    "                item_for_output.pop(key_to_remove, None)\n",
    "\n",
    "            processed_data.append(item_for_output)\n",
    "\n",
    "    print(\n",
    "        f\"Filtering complete. Kept {len(processed_data)} entries, removed {filtered_out_count} entries.\")\n",
    "\n",
    "    print(f\"Saving filtered and cleaned data to '{output_json_file}'...\")\n",
    "    with open(output_json_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=2)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "    print(\"\\n--- Analyzing Final Data ---\")\n",
    "    final_total_word_count = 0\n",
    "    final_total_instruction_words = 0\n",
    "    final_instruction_set_count = 0\n",
    "\n",
    "    if not processed_data:\n",
    "        print(\"No data remaining after filtering to analyze.\")\n",
    "    else:\n",
    "        for item in processed_data:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "\n",
    "            item_word_count = 0\n",
    "            item_word_count += count_words(item.get(\"title\"))\n",
    "\n",
    "            location = item.get(\"location\")\n",
    "            if isinstance(location, dict):\n",
    "                item_word_count += count_words(location.get(\"province\"))\n",
    "                item_word_count += count_words(location.get(\"city\"))\n",
    "\n",
    "            ingredients = item.get(\"ingredients\")\n",
    "            if isinstance(ingredients, list):\n",
    "                for ingredient in ingredients:\n",
    "                    if isinstance(ingredient, dict):\n",
    "                        item_word_count += count_words(ingredient.get(\"name\"))\n",
    "                        item_word_count += count_words(ingredient.get(\"unit\"))\n",
    "\n",
    "            instructions = item.get(\"instructions\")\n",
    "            current_instruction_words = 0\n",
    "            has_valid_instructions = False\n",
    "            if isinstance(instructions, list):\n",
    "                for instruction_step in instructions:\n",
    "                    step_words = count_words(instruction_step)\n",
    "                    item_word_count += step_words  \n",
    "                    current_instruction_words += step_words\n",
    "                if current_instruction_words > 0:\n",
    "                    has_valid_instructions = True\n",
    "\n",
    "            item_word_count += count_words(item.get(\"meal_type\"))\n",
    "\n",
    "            occasion = item.get(\"occasion\")\n",
    "            if isinstance(occasion, dict):\n",
    "                choices = occasion.get(\"choices\")\n",
    "                if isinstance(choices, list):\n",
    "                    for choice in choices:\n",
    "                        item_word_count += count_words(choice)\n",
    "\n",
    "            final_total_word_count += item_word_count\n",
    "\n",
    "            if has_valid_instructions:\n",
    "                final_total_instruction_words += current_instruction_words\n",
    "                final_instruction_set_count += 1\n",
    "\n",
    "        mean_instruction_length = (final_total_instruction_words / final_instruction_set_count) \\\n",
    "            if final_instruction_set_count > 0 else 0\n",
    "\n",
    "        print(\n",
    "            f\"Total word count in '{output_json_file}': {final_total_word_count}\")\n",
    "        print(\n",
    "            f\"Average instruction length in '{output_json_file}' (words per recipe with instructions): {mean_instruction_length:.2f}\")\n",
    "        print(\n",
    "            f\"Number of recipes with instructions considered for average: {final_instruction_set_count}\")\n",
    "\n",
    "\n",
    "# --- Error Handling ---\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Input file '{input_json_file}' not found.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\n",
    "        f\"Error: Could not decode JSON from '{input_json_file}'. Invalid JSON format. Details: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {type(e).__name__} - {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()  # Print detailed traceback for unexpected errors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
